import cmd
from scraping import *
import pickle
import os
import urllib.request as req
from Data_Methods import Data_Methods


class HelpCMD(cmd.Cmd, Webscraping):

    """
    Web Scraping CMD
    """
    global new_url

    def __init__(self):
        cmd.Cmd.__init__(self)
        self.prompt = ">>>"
        self.cmdloop()

    def do_seturl(self, webscraping):
        """
        Set URL of Fishpond page to scrap information from
        :return: Input
        """
        set_url()

    def do_product(self, webscraping):
        """
        List of all scraped product names, as list
        :return: Product Name
         """
        print("Product name: ", Webscraping.product_name(self))

    def do_isbn(self, webscraping):
        """
        List of all scraped product ISBN's, as list
        :return: Book ISBN
        """
        print("ISBN:  ", Webscraping.isbn(self))

    def do_savings(self, webscraping):
        """
        List of all scraped savings, as list
        :return: Savings
        """
        print('Savings : ', Webscraping.saving_total(self))

    def do_publish(self, webscraping):
        """
        List of all scraped product's Publishing Date, as list
        :return: Publishing Date
        """
        print('Publishing Date: ', Webscraping.publishing_date(self))

    def do_rrp(self, webscraping):
        """
        List of all scraped product RRP, as list
        :return: RRP
        """
        print('Normal Price ($): ', Webscraping.RRP(self))

    def do_sale(self, webscraping):
        """
        List of all scraped product's Sale Price, as list
        :return: Sale Price
        """
        print('Sale Price ($): ', Webscraping.sale_prices(self))

    def do_image(self, webscraping):
        """
        List of all scraped product's relevant image, as url list
        :return: Product's Image
        """
        print('Product Photo: ', Webscraping.photo_link(self))

    def do_pricecomparison(self, webscraping):
        """
        Graphical comparison of RRP and Sale Price for all products
        :return: Graph
        """
        Data_Methods.price_comparison(self)

    def do_totalsavingsdata(self, webscraping):
        """
        Graph showing the savings (%) of all the products scraped
        :return: Graph
        """
        Data_Methods.total_savings_data(self)

    def do_publishingdata(self, webscraping):
        """
        Pie chart showing a breakdown publishing dates by month
        :return: Graph
        """
        Data_Methods.publishing_data(self)

    def do_quit(self, webscraping):
        """
        Quit from my CMD
        :return: True
        """
        print("Quitting ......")
        return True

    def help_quit(self, webscraping):
        print('\n'.join(['Quit from my CMD', ':return: True']))

    def do_savefile(self, webscraping):
        """
        Saves information scraped to txt file
        :data types: Product Name, ISBN, Publish Date, Normal Price,
                    Sale Price, Savings, Photos, All
        :return: None
        """
        results = Webscraping()
        save_path = input("What is the directory?: ")
        try:
            name_of_file = input("What is the name of the file: ")
            type = input("What data type do you want to save?: ")
            if type == 'Product Name':
                save = results.product_name()
            elif type == 'ISBN':
                save = results.isbn()
            elif type == "Publish Date":
                save = results.publishing_date()
            elif type == 'Normal Price':
                save = results.RRP()
            elif type == 'Sale Price':
                save = results.sale_prices()
            elif type == 'Savings':
                save = results.saving_total()
            elif type == 'Photos':
                save = results.photo_link()
            elif type == "Save all":
                save = (results.product_name(), results.isbn(), results.RRP(),
                        results.sale_prices(),
                        results.saving_total(), results.photo_link())
            complete_name = os.path.join(save_path, name_of_file + ".txt")
            file1 = open(complete_name, "wb")
            pickle.dump(save, file1)
            file1.close()
        except FileNotFoundError:
                print("Please enter a valid save directory")

    def do_loadfile(self, webscraping):
        """
        Loads data from previously saved file
        :return: Data
        """
        infile = input("Please enter the file path: ")
        try:
            f = open(infile, 'rb')
            results = pickle.load(f)
            print(results)
            f.close()
        except FileNotFoundError:
            print("Please enter a valid file path")

if __name__ == '__main__':
    prompt = HelpCMD()
